"""
Content Moderation Service using OpenAI's omni-moderation-latest

This service provides industry-grade content moderation for forum posts and replies
using OpenAI's most advanced moderation model (omni-moderation-latest).

MULTILINGUAL SUPPORT:
- Supports 40+ languages including English, Filipino/Tagalog, and Taglish
- 42% improvement in multilingual performance vs previous models
- 70% improvement for low-resource languages
- No special configuration needed - automatically detects language

INDUSTRY BEST PRACTICES:
- Fail-open strategy (high availability over strict blocking)
- Configurable thresholds per category
- Comprehensive logging for audit trails
- User-friendly error messages
- Singleton pattern for performance
- Async/await for non-blocking operations

Features:
- Multi-modal content analysis (text, images, etc.)
- Category-specific flagging (hate, harassment, self-harm, sexual, violence)
- Severity scoring for each category (0.0 to 1.0)
- Detailed violation reports
- Configurable thresholds via environment variables
- Production-ready error handling

Usage:
    from services.content_moderation_service import ContentModerationService
    
    moderation = ContentModerationService()
    
    # Works with any language (English, Filipino, Tagalog, Taglish, etc.)
    result = await moderation.moderate_content("User post content here")
    
    if result["flagged"]:
        # Handle policy violation
        categories = result["categories"]
        severity = result["category_scores"]

References:
- OpenAI Moderation API: https://platform.openai.com/docs/guides/moderation
- Industry Standards: Discord, Reddit, Slack moderation approaches
- Compliance: Philippine RA 10175, RA 11313, RA 9775, RA 10173
"""

from openai import AsyncOpenAI
import os
from typing import Dict, Any, List, Optional
import logging
from dotenv import load_dotenv
from services.filipino_profanity_filter import get_filipino_profanity_filter
from services.safety_filter import get_safety_filter

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)

class ContentModerationService:
    """
    Content moderation service using OpenAI's omni-moderation-latest model.
    
    MULTILINGUAL SUPPORT:
    This service automatically handles content in multiple languages including:
    - English (primary)
    - Filipino/Tagalog (native support)
    - Taglish (mixed English-Tagalog)
    - 40+ other languages
    
    The omni-moderation-latest model has been specifically trained on multilingual
    data with 42% improvement in non-English language performance compared to
    previous models. No special configuration is needed - the model automatically
    detects and processes the language.
    
    INDUSTRY BEST PRACTICES:
    - Fail-open strategy: If moderation fails, allow content (availability > strict blocking)
    - Configurable thresholds: Adjust sensitivity per category via environment variables
    - Comprehensive logging: All events logged with severity levels and emojis
    - Singleton pattern: Single instance reused across requests for efficiency
    - Async operations: Non-blocking API calls for better performance
    
    This service uses the same OpenAI API key as the chatbot for consistency.
    """
    
    def __init__(self):
        """Initialize the moderation service with OpenAI client."""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("OPENAI_API_KEY not found in environment variables")
            raise ValueError("OPENAI_API_KEY must be set for content moderation")
        
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = "omni-moderation-latest"
        self.filipino_filter = get_filipino_profanity_filter()
        self.safety_filter = get_safety_filter()
        
        # STRICT THRESHOLDS - Zero tolerance for harmful content
        # Lower values = more sensitive detection
        self.thresholds = {
            "hate": float(os.getenv("MODERATION_THRESHOLD_HATE", "0.05")),  # VERY STRICT - catches "I hate all lawyers"
            "hate/threatening": float(os.getenv("MODERATION_THRESHOLD_HATE_THREATENING", "0.05")),  # VERY STRICT
            "harassment": float(os.getenv("MODERATION_THRESHOLD_HARASSMENT", "0.3")),  # More sensitive
            "harassment/threatening": float(os.getenv("MODERATION_THRESHOLD_HARASSMENT_THREATENING", "0.1")),  # Very sensitive
            "self-harm": float(os.getenv("MODERATION_THRESHOLD_SELF_HARM", "0.1")),  # Very sensitive
            "self-harm/intent": float(os.getenv("MODERATION_THRESHOLD_SELF_HARM_INTENT", "0.1")),  # Very sensitive
            "self-harm/instructions": float(os.getenv("MODERATION_THRESHOLD_SELF_HARM_INSTRUCTIONS", "0.1")),  # Very sensitive
            "sexual": float(os.getenv("MODERATION_THRESHOLD_SEXUAL", "0.25")),  # More sensitive
            "sexual/minors": float(os.getenv("MODERATION_THRESHOLD_SEXUAL_MINORS", "0.01")),  # CRITICAL: Near-zero tolerance
            "violence": float(os.getenv("MODERATION_THRESHOLD_VIOLENCE", "0.3")),  # More sensitive
            "violence/graphic": float(os.getenv("MODERATION_THRESHOLD_VIOLENCE_GRAPHIC", "0.2")),  # More sensitive
        }
        
        logger.info(f"âœ… Content moderation service initialized with model: {self.model}")
        logger.info(f"ðŸŒ Multilingual support: 40+ languages (English, Filipino/Tagalog, Taglish, etc.)")
        logger.info(f"ðŸ›¡ï¸  Fail-open strategy enabled for high availability")
    
    async def moderate_content(self, content: str) -> Dict[str, Any]:
        """
        Moderate content using OpenAI's omni-moderation-latest model.
        
        MULTILINGUAL: This method automatically handles content in any language
        including English, Filipino, Tagalog, Taglish, and 40+ other languages.
        No special configuration or language detection is needed.
        
        Args:
            content: The text content to moderate (forum post or reply)
                    Can be in any language - English, Filipino, Tagalog, Taglish, etc.
        
        Returns:
            Dict containing:
                - flagged (bool): Whether content violates policies
                - categories (dict): Which categories were flagged
                - category_scores (dict): Confidence scores for each category (0.0-1.0)
                - violation_summary (str): Human-readable summary of violations
                - raw_response (dict): Full OpenAI moderation response
        
        Raises:
            Exception: If moderation API call fails
        
        Examples:
            # English content
            result = await moderate_content("I hate all lawyers")
            
            # Filipino/Tagalog content
            result = await moderate_content("Galit ako sa lahat ng abogado")
            
            # Taglish content
            result = await moderate_content("Ang dami kong galit sa mga lawyers")
        """
        try:
            # STEP 0: Safety filter - ZERO TOLERANCE for child safety, abuse, harassment
            safety_check = self.safety_filter.analyze(content)
            
            if safety_check["is_unsafe"]:
                logger.error(f"ðŸš¨ SAFETY VIOLATION: {safety_check['violations']} (severity: {safety_check['severity']})")
                # Immediate flag for safety violations
                return {
                    "flagged": True,
                    "categories": {
                        "harassment": True,
                        "hate": safety_check["severity"] == "critical",
                        "hate/threatening": safety_check["severity"] == "critical",
                        "harassment/threatening": safety_check["severity"] in ["critical", "high"],
                        "self-harm": "abuse_pattern" in safety_check["violations"],
                        "self-harm/intent": False,
                        "self-harm/instructions": False,
                        "sexual": "sexual_harassment" in safety_check["violations"],
                        "sexual/minors": "child_sexualization" in safety_check["violations"] or "grooming_behavior" in safety_check["violations"],
                        "violence": "veiled_threat" in safety_check["violations"],
                        "violence/graphic": False
                    },
                    "category_scores": {
                        "harassment": 0.99,
                        "hate": 0.95 if safety_check["severity"] == "critical" else 0.80,
                        "hate/threatening": 0.90 if safety_check["severity"] == "critical" else 0.0,
                        "harassment/threatening": 0.95 if safety_check["severity"] in ["critical", "high"] else 0.0,
                        "self-harm": 0.85 if "abuse_pattern" in safety_check["violations"] else 0.0,
                        "self-harm/intent": 0.0,
                        "self-harm/instructions": 0.0,
                        "sexual": 0.90 if "sexual_harassment" in safety_check["violations"] else 0.0,
                        "sexual/minors": 0.99 if "child_sexualization" in safety_check["violations"] or "grooming_behavior" in safety_check["violations"] else 0.0,
                        "violence": 0.85 if "veiled_threat" in safety_check["violations"] else 0.0,
                        "violence/graphic": 0.0
                    },
                    "violation_summary": safety_check["message"],
                    "raw_response": {
                        "flagged": True,
                        "source": "safety_filter",
                        "violations": safety_check["violations"],
                        "severity": safety_check["severity"]
                    }
                }
            
            # STEP 1: Check for Filipino profanity (catches what OpenAI might miss)
            profanity_check = self.filipino_filter.contains_profanity(content)
            
            if profanity_check["is_profane"]:
                logger.warning(f"ðŸš¨ Filipino profanity detected: {profanity_check['matched_words']}")
                # Return flagged result immediately
                return {
                    "flagged": True,
                    "categories": {
                        "harassment": True,
                        "hate": profanity_check["severity"] == "high",
                        "hate/threatening": False,
                        "harassment/threatening": profanity_check["severity"] == "high",
                        "self-harm": False,
                        "self-harm/intent": False,
                        "self-harm/instructions": False,
                        "sexual": False,
                        "sexual/minors": False,
                        "violence": False,
                        "violence/graphic": False
                    },
                    "category_scores": {
                        "harassment": 0.95 if profanity_check["severity"] == "high" else 0.85,
                        "hate": 0.90 if profanity_check["severity"] == "high" else 0.75,
                        "hate/threatening": 0.0,
                        "harassment/threatening": 0.85 if profanity_check["severity"] == "high" else 0.0,
                        "self-harm": 0.0,
                        "self-harm/intent": 0.0,
                        "self-harm/instructions": 0.0,
                        "sexual": 0.0,
                        "sexual/minors": 0.0,
                        "violence": 0.0,
                        "violence/graphic": 0.0
                    },
                    "violation_summary": self.filipino_filter.get_violation_message(
                        profanity_check["matched_words"],
                        profanity_check["severity"]
                    ),
                    "raw_response": {
                        "flagged": True,
                        "source": "filipino_profanity_filter",
                        "matched_words": profanity_check["matched_words"],
                        "severity": profanity_check["severity"]
                    }
                }
            
            # STEP 2: Call OpenAI Moderation API for additional checks
            logger.info(f"ðŸ” Moderating content with OpenAI (length: {len(content)} chars)")
            
            response = await self.client.moderations.create(
                model=self.model,
                input=content
            )
            
            # Extract moderation results
            result = response.results[0]
            
            # Apply custom thresholds instead of using OpenAI's default flagged status
            category_scores = result.category_scores.model_dump()
            
            # DEBUG: Log all category scores
            logger.info(f"ðŸ“Š Category Scores:")
            for category, score in category_scores.items():
                threshold = self.thresholds.get(category, 0.5)
                logger.info(f"   {category}: {score:.4f} (threshold: {threshold})")
            
            custom_flagged = self._apply_custom_thresholds(category_scores)
            
            # Build detailed response with custom flagged status
            moderation_result = {
                "flagged": custom_flagged,
                "categories": result.categories.model_dump(),
                "category_scores": category_scores,
                "violation_summary": self._generate_violation_summary_from_dict(category_scores, custom_flagged),
                "raw_response": result.model_dump()
            }
            
            # Log results
            if custom_flagged:
                logger.warning(f"âš ï¸  Content flagged: {moderation_result['violation_summary']}")
            else:
                logger.info("âœ… Content passed moderation")
            
            return moderation_result
            
        except Exception as e:
            logger.error(f"âŒ Content moderation failed: {str(e)}")
            raise Exception(f"Content moderation error: {str(e)}")
    
    async def moderate_batch(self, contents: List[str]) -> List[Dict[str, Any]]:
        """
        Moderate multiple pieces of content in a single API call.
        
        MULTILINGUAL: Each content item can be in a different language.
        The model automatically handles mixed-language batches.
        
        Args:
            contents: List of text contents to moderate (can be in different languages)
        
        Returns:
            List of moderation results (same format as moderate_content)
        
        Example:
            results = await moderate_batch([
                "English content here",
                "Filipino content dito",
                "Taglish mixed content"
            ])
        """
        try:
            logger.info(f"ðŸ” Batch moderating {len(contents)} items")
            
            response = await self.client.moderations.create(
                model=self.model,
                input=contents
            )
            
            results = []
            for result in response.results:
                moderation_result = {
                    "flagged": result.flagged,
                    "categories": result.categories.model_dump(),
                    "category_scores": result.category_scores.model_dump(),
                    "violation_summary": self._generate_violation_summary(result),
                    "raw_response": result.model_dump()
                }
                results.append(moderation_result)
            
            flagged_count = sum(1 for r in results if r["flagged"])
            logger.info(f"âœ… Batch moderation complete: {flagged_count}/{len(contents)} flagged")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Batch moderation failed: {str(e)}")
            raise Exception(f"Batch moderation error: {str(e)}")
    
    def _apply_custom_thresholds(self, category_scores: Dict[str, float]) -> bool:
        """
        Apply custom thresholds to category scores to determine if content should be flagged.
        
        Args:
            category_scores: Dictionary of category scores from OpenAI
        
        Returns:
            True if any category exceeds its threshold, False otherwise
        """
        for category, score in category_scores.items():
            threshold = self.thresholds.get(category, 0.5)
            if score >= threshold:
                logger.info(f"ðŸš¨ Category '{category}' exceeded threshold: {score:.3f} >= {threshold}")
                return True
        return False
    
    def _generate_violation_summary_from_dict(self, category_scores: Dict[str, float], is_flagged: bool) -> str:
        """
        Generate a human-readable summary of policy violations from category scores.
        
        Args:
            category_scores: Dictionary of category scores
            is_flagged: Whether content was flagged
        
        Returns:
            String describing which policies were violated
        """
        if not is_flagged:
            return "No violations detected"
        
        violations = []
        
        # Check each category against thresholds
        for category, score in category_scores.items():
            threshold = self.thresholds.get(category, 0.5)
            if score >= threshold:
                # Format category name for readability
                category_name = category.replace("-", " ").replace("/", " - ").title()
                violations.append(f"{category_name} (score: {score:.2f}, threshold: {threshold})")
        
        if violations:
            return "Violations: " + ", ".join(violations)
        else:
            return "Flagged but no specific violations above threshold"
    
    def _generate_violation_summary(self, result) -> str:
        """
        Generate a human-readable summary of policy violations.
        
        Args:
            result: OpenAI moderation result object
        
        Returns:
            String describing which policies were violated
        """
        if not result.flagged:
            return "No violations detected"
        
        violations = []
        categories = result.categories.model_dump()
        scores = result.category_scores.model_dump()
        
        # Check each category against thresholds
        for category, is_flagged in categories.items():
            if is_flagged:
                score = scores.get(category, 0)
                threshold = self.thresholds.get(category, 0.5)
                
                # Format category name for readability
                category_name = category.replace("-", " ").replace("/", " - ").title()
                violations.append(f"{category_name} (score: {score:.2f}, threshold: {threshold})")
        
        if violations:
            return "Violations: " + ", ".join(violations)
        else:
            return "Flagged but no specific violations above threshold"
    
    def is_content_safe(self, moderation_result: Dict[str, Any]) -> bool:
        """
        Determine if content is safe to post based on moderation results.
        
        Args:
            moderation_result: Result from moderate_content()
        
        Returns:
            True if content is safe, False if it should be blocked
        """
        return not moderation_result["flagged"]
    
    def get_violation_message(self, moderation_result: Dict[str, Any], context: str = "post") -> str:
        """
        Get a user-friendly message explaining why content was blocked.
        
        Args:
            moderation_result: Result from moderate_content()
            context: Context of the violation - "post" (forum), "reply" (forum), or "chatbot"
        
        Returns:
            User-friendly error message appropriate for the context
        """
        if not moderation_result["flagged"]:
            return ""
        
        categories = moderation_result["categories"]
        category_scores = moderation_result.get("category_scores", {})
        
        # Get flagged categories with their scores
        flagged_with_scores = [
            (k, category_scores.get(k, 0.0)) 
            for k, v in categories.items() if v
        ]
        
        if not flagged_with_scores:
            if context == "chatbot":
                return "Your message violates our community guidelines. Please be mindful of your language when interacting with the chatbot."
            else:
                return "Your content violates our community guidelines. Please revise and try again."
        
        # Get the PRIMARY category (highest score) to avoid overwhelming the user
        primary_category, _ = max(flagged_with_scores, key=lambda x: x[1])
        primary_category_name = primary_category.replace("-", " ").replace("/", " - ").title()
        
        # Simplify category names for better readability
        category_simplifications = {
            "Harassment - Threatening": "Threatening Behavior",
            "Self Harm - Intent": "Self-Harm Content",
            "Self Harm - Instructions": "Self-Harm Content",
            "Violence - Graphic": "Violent Content",
            "Hate - Threatening": "Hate Speech",
            "Sexual - Minors": "Inappropriate Content"
        }
        
        primary_category_name = category_simplifications.get(primary_category_name, primary_category_name)
        
        # Generate message with only the primary category
        if context == "chatbot":
            return f"Your message was flagged for {primary_category_name}. Please be mindful of your language when using the chatbot."
        else:
            return f"Your content was flagged for {primary_category_name}. Please revise your post to comply with our community guidelines."


# Singleton instance for reuse across the application
_moderation_service_instance: Optional[ContentModerationService] = None

def get_moderation_service() -> ContentModerationService:
    """
    Get or create the singleton moderation service instance.
    
    Returns:
        ContentModerationService instance
    """
    global _moderation_service_instance
    
    if _moderation_service_instance is None:
        _moderation_service_instance = ContentModerationService()
    
    return _moderation_service_instance
