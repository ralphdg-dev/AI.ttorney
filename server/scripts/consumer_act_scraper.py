# Disclaimer: This script scrapes publicly available information from LawPhil for educational and research purposes only. 
# It does not provide legal advice or create a lawyer-client relationship.

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import re
from pathlib import Path

# URL to scrape
CONSUMER_ACT_URL = "https://lawphil.net/statutes/repacts/ra1992/ra_7394_1992.html"

def clean_text(text):
    """Cleans and normalizes text by removing extra whitespace."""
    return re.sub(r'\s+', ' ', text.strip())

def classify_article_by_content(content, title="", number=""):
    """Intelligently classify articles based on content keywords"""
    content_lower = (content + " " + title).lower()
    
    # Keywords for Consumer Rights (Title II - right to safety, information, redress)
    consumer_rights_keywords = [
        'rights', 'right to', 'safety', 'information', 'redress', 'remedy', 'protection',
        'consumer rights', 'basic rights', 'welfare', 'health', 'security', 'quality',
        'standard', 'warranty', 'guarantee', 'complaint', 'grievance', 'compensation'
    ]
    
    # Keywords for Unfair Sales Practices (Title IV - deceptive, misleading, unfair marketing)
    unfair_sales_keywords = [
        'unfair', 'deceptive', 'misleading', 'false', 'fraud', 'misrepresentation',
        'advertising', 'promotion', 'marketing', 'sales', 'selling', 'offer',
        'price manipulation', 'bait and switch', 'pyramid', 'chain letter',
        'unconscionable', 'prohibited acts', 'unlawful', 'violation'
    ]
    
    # Count keyword matches
    scores = {
        'Consumer Rights': sum(1 for keyword in consumer_rights_keywords if keyword in content_lower),
        'Unfair Sales Practices': sum(1 for keyword in unfair_sales_keywords if keyword in content_lower)
    }
    
    # Get the topic with highest score
    primary_topic = max(scores, key=scores.get)
    
    # If no clear winner, use title-based classification
    if scores[primary_topic] == 0:
        # Check if it's in Title II (Consumer Rights) or Title IV (Unfair Sales Practices)
        if 'title ii' in content_lower or 'title 2' in content_lower:
            return 'Consumer Rights'
        elif 'title iv' in content_lower or 'title 4' in content_lower:
            return 'Unfair Sales Practices'
        else:
            # Default classification based on article number ranges
            try:
                num = int(number)
                if 1 <= num <= 50:
                    return 'Consumer Rights'
                else:
                    return 'Unfair Sales Practices'
            except:
                return 'Consumer Rights'
    
    return primary_topic

def scrape_consumer_act(url):
    """Scrape Consumer Act from LawPhil"""
    print("ðŸ”„ Fetching Consumer Act content...")
    response = requests.get(url)
    response.encoding = 'utf-8'
    soup = BeautifulSoup(response.text, "html.parser")

    all_paragraphs = soup.find_all("p", attrs={'align': 'justify'})
    
    current_title = ""
    current_chapter = ""
    articles = []
    
    for p in all_paragraphs:
        text = clean_text(p.get_text(" ", strip=True))
        if not text:
            continue

        # Detect TITLE
        title_match = re.match(r"TITLE\s+([IVXLC]+)\s*[-â€“â€”]?\s*(.+)", text, re.I)
        if title_match:
            title_number = title_match.group(1)
            title_name = title_match.group(2).strip()
            current_title = f"TITLE {title_number} â€” {title_name}"
            print(f"ðŸ“– Found: {current_title}")
            continue
        
        # Also check for standalone TITLE lines
        if re.match(r"^TITLE\s+[IVXLC]+\s*$", text, re.I):
            current_title = text.strip()
            print(f"ðŸ“– Found: {current_title}")
            continue

        # Detect Chapter
        chapter_match = re.match(r"Chapter\s+(\d+)\.\s*(.+)", text)
        if chapter_match:
            chapter_number = chapter_match.group(1)
            chapter_name = chapter_match.group(2).strip()
            current_chapter = f"Chapter {chapter_number}. {chapter_name}"
            print(f"ðŸ“‘ Found: {current_chapter}")
            continue

        # Detect Article
        article_match = re.match(r"Article\s+(\d+)\.?\s*(.*)", text, re.DOTALL)
        if article_match:
            article_number = article_match.group(1)
            content = text
            
            # Extract title (first sentence)
            content_after_number = article_match.group(2).strip()
            title_match = re.match(r'^([^.]+)', content_after_number)
            if title_match:
                title = clean_text(title_match.group(1))
            else:
                title = f"Article {article_number}"
            
            # Classify by content
            topic = classify_article_by_content(content, title, article_number)
            
            article = {
                "law": "Republic Act No. 7394",
                "type": "Article",
                "number": article_number,
                "title": title,
                "content": content,
                "topic": topic,
                "hierarchy": {
                    "title": current_title,
                    "chapter": current_chapter
                }
            }
            
            articles.append(article)
            print(f"ðŸ“„ Article {article_number} - {topic}: {title[:50]}...")
            if current_title or current_chapter:
                print(f"    â””â”€ Hierarchy: {current_title} | {current_chapter}")
    
    return articles

def organize_by_topics(all_articles):
    """Organize all articles by the two main topics"""
    topics = {
        "Consumer Rights": [],
        "Unfair Sales Practices": []
    }
    
    for article in all_articles:
        topic = article['topic']
        topics[topic].append(article)
    
    return topics

def create_consumer_data(consumer_articles):
    """Create clean comprehensive JSON structure"""
    topics_data = organize_by_topics(consumer_articles)
    
    data = {
        "title": "THE CONSUMER ACT OF THE PHILIPPINES",
        "description": "Republic Act No. 7394 - Consumer Protection Law",
        "law": {
            "name": "Republic Act No. 7394",
            "full_name": "The Consumer Act of the Philippines",
            "date": "April 13, 1992",
            "url": CONSUMER_ACT_URL
        },
        "topics": topics_data,
        "statistics": {
            "total_articles": len(consumer_articles),
            "by_topic": {topic: len(articles) for topic, articles in topics_data.items()}
        },
        "metadata": {
            "date_accessed": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "source": CONSUMER_ACT_URL
        }
    }
    
    return data

def save_json(data, filename="consumer_act.json"):
    """Save data as JSON file"""
    # Ensure data/raw directory exists
    output_dir = Path(__file__).parent.parent / "data" / "raw"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Save to data/raw directory
    output_path = output_dir / filename
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"âœ… JSON saved to {output_path}")

def save_markdown(data, filename="consumer_act.md"):
    """Save data as Markdown file"""
    # Save markdown to data/raw directory as well
    output_dir = Path(__file__).parent.parent / "data" / "raw"
    output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / filename
    
    md = []
    md.append(f"# {data['title']}\n")
    md.append(f"{data['description']}\n")
    
    # Law section
    law = data['law']
    md.append("## Law Details\n")
    md.append(f"- **{law['name']}** ({law['full_name']})")
    md.append(f"- **Date:** {law['date']}")
    md.append(f"- **Source:** {law['url']}\n")
    
    # Statistics
    stats = data['statistics']
    md.append("## Statistics\n")
    md.append(f"- **Total Articles:** {stats['total_articles']}\n")
    
    md.append("### By Topic:\n")
    for topic, count in stats['by_topic'].items():
        md.append(f"- **{topic}:** {count} articles")
    
    # Topics content
    for topic_name, articles in data['topics'].items():
        md.append(f"\n## {topic_name}\n")
        
        for article in articles:
            md.append(f"\n### {article['law']} - {article['type']} {article['number']}")
            md.append(f"**{article['title']}**\n")
            
            if article['hierarchy']['title']:
                md.append(f"*{article['hierarchy']['title']}*")
            if article['hierarchy']['chapter']:
                md.append(f" | *{article['hierarchy']['chapter']}*")
            md.append("\n")
            
            md.append(f"{article['content']}\n")
    
    md.append(f"\n---\n**Date Accessed:** {data['metadata']['date_accessed']}")

    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(md))
    print(f"âœ… Markdown saved to {output_path}")

def display_summary(data):
    """Display clean summary"""
    print(f"\nðŸ“Š **EXTRACTION SUMMARY**")
    print("=" * 50)
    print(f"ðŸ“– {data['title']}")
    print(f"ðŸ“ {data['description']}")
    
    law = data['law']
    print(f"\nðŸ“š **Law:** {law['name']} ({law['date']})")
    
    stats = data['statistics']
    print(f"\nðŸ“„ **Total Articles:** {stats['total_articles']}")
    
    print(f"\nðŸ·ï¸  **By Topic:**")
    for topic, count in stats['by_topic'].items():
        print(f"  â€¢ {topic}: {count} articles")
        
        # Show sample articles
        sample_articles = data['topics'][topic][:2]
        for article in sample_articles:
            print(f"    - Article {article['number']}: {article['title'][:50]}...")

if __name__ == "__main__":
    print("ðŸ›ï¸  CONSUMER ACT SCRAPER")
    print("ðŸ“š Republic Act No. 7394")
    print("=" * 35)
    
    # Scrape Consumer Act
    consumer_articles = scrape_consumer_act(CONSUMER_ACT_URL)
    
    # Create clean data structure
    consumer_data = create_consumer_data(consumer_articles)
    
    # Save outputs
    save_json(consumer_data)
    save_markdown(consumer_data)
    
    # Display summary
    display_summary(consumer_data)
    
    print("\nâœ… **CONSUMER ACT SCRAPING COMPLETE!**")
    print("ðŸ“„ Files generated:")
    print("  â€¢ consumer_act.json - Clean structured data")
    print("  â€¢ consumer_act.md - Readable documentation")
    print(f"ðŸ”— Source: {CONSUMER_ACT_URL}")
